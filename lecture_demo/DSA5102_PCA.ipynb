{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "\n",
    "In this notebook, we will explore the application of PCA for a variety of practical problems, including\n",
    "   * Feature extraction and dimensionality reduction\n",
    "   * Denoising\n",
    "   * Clustering and measuring similarity\n",
    "   \n",
    "We will also demonstrate basic usage of `sklearn.decomposition.PCA`, as well as building autoencoders with `keras`. Incidentally, we will also use the `kaggle` API to download some public datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('notebook', font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.random.seed(123)  # For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and Autoecnoders on MNIST\n",
    "\n",
    "In this section, we perform principal component analysis and autoencoders on the MNIST hand-written digits dataset that we have seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(x, nrow=2, ncol=4, randomize=True):\n",
    "    \"\"\"\n",
    "    Plot Images\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(nrow, ncol, figsize=(5*ncol, 4*nrow))\n",
    "    num_samples = x.shape[0]\n",
    "    if x.ndim == 2:\n",
    "        img_size = int(round(np.sqrt(x.shape[-1])))\n",
    "        assert img_size**2 == x.shape[-1]  # Check Square Image\n",
    "        x = x.reshape(-1, img_size, img_size)\n",
    "    for k, a in enumerate(ax.ravel()):\n",
    "        if randomize:\n",
    "            j = np.random.choice(num_samples)\n",
    "        else:\n",
    "            j = k\n",
    "        a.imshow(x[j])\n",
    "        a.set_xticks([])\n",
    "        a.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the data using `keras`. You can also directly download it from the [web](http://yann.lecun.com/exdb/mnist/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets.mnist import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(images_train, _), (images_test, _) = load_data()\n",
    "images_train = images_train / 255.0\n",
    "images_test = images_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(images_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now perform PCA. For demonstration we will only work with the training set and evaluate our performance on the test set. In general, depending on the task at hand this splitting may be unnecesssary. For example, if we are interested in compression or feature extraction then there is no need for a train-test split.\n",
    "\n",
    "Here, we will use the `sklearn.decomposition.PCA` class to perform PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=64)\n",
    "pca.fit(images_train.reshape(-1, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues / Explained Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we plot the eigenvalues of the sample covariance matrix (equivalently, the singular values of the data matrix). Note that they are arranged in decreasing order and accessible from the `explained_variance_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_)\n",
    "plt.xlabel(r'$j$')\n",
    "plt.ylabel(r'$\\lambda_j$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see what principal component axes are identified. These are accessible through the `components_` attribute. \n",
    "\n",
    "**Remark.**\n",
    "Unlike our notation in the notes, here each eigenvector is a *row*, instead of a *column* of the matrix `pca.components_`. In other words, `pca.components_(j)` is the $j^\\text{th}$ eigenvector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(pca.components_, randomize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction using Principal Component Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrate how to get back our image. As discussed, the PCA compression gives the rescaled latents\n",
    "$$\n",
    "    Z_m = X U_m \\Lambda^{-1/2}\n",
    "$$\n",
    "Here, the latents are computed by calling `pca.transform`.\n",
    "\n",
    "The inverse transform is given by\n",
    "$$\n",
    "    X'_m = Z_m U_m^T \\Lambda^{-1/2}\n",
    "$$\n",
    "This gives us the reconstructed image. Here, it is computed using `pca.inverse_transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pca.transform(images_test.reshape(-1, 784))\n",
    "images_recon = pca.inverse_transform(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how well we did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(images_test, randomize=False, nrow=1)\n",
    "plot_images(images_recon, randomize=False, nrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder with Neural Networks\n",
    "\n",
    "Let us now try to do the same compression, but with a nonlinear neural network as encoder and decoder functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "    x = Dense(units=128, activation='relu')(x)\n",
    "    x = Dense(units=64, activation='relu')(x)\n",
    "    return x\n",
    "\n",
    "def decoder(x):\n",
    "    x = Dense(units=128, activation='relu')(x)\n",
    "    x = Dense(units=784, activation='sigmoid')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(784,))\n",
    "z = encoder(x)\n",
    "x_recon = decoder(z)\n",
    "\n",
    "autoencoder = Model(inputs=x, outputs=x_recon)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(\n",
    "    x=images_train.reshape(-1, 784),\n",
    "    y=images_train.reshape(-1, 784),\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_recon_ae = autoencoder.predict(x=images_test.reshape(-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(images_test, randomize=False, nrow=1)\n",
    "plot_images(images_recon_ae, randomize=False, nrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising using PCA and AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us see another use for PCA and Autoencoders: denoising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_and_clip(x):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to images\n",
    "    \"\"\"\n",
    "    x = x + 0.2 * np.random.normal(size=x.shape)\n",
    "    x = np.maximum(x, 0.0)\n",
    "    x = np.minimum(x, 1.0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create some noisy images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train_noisy = add_noise_and_clip(images_train)\n",
    "images_test_noisy = add_noise_and_clip(images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(images_train, randomize=False, nrow=1)\n",
    "plot_images(images_train_noisy, randomize=False, nrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the PCA, we can transform the noisy image into the latent space and then back. Because this encoding-decoding scheme is lossy, it actually drops many unimportant features in the data, including the noise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pca.transform(images_test_noisy.reshape(-1, 784))\n",
    "images_recon = pca.inverse_transform(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(images_test, randomize=False, nrow=1)\n",
    "plot_images(images_test_noisy, randomize=False, nrow=1)\n",
    "plot_images(images_recon, randomize=False, nrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same with autoencoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_recon_ae = autoencoder.predict(x=images_test_noisy.reshape(-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(images_test, randomize=False, nrow=1)\n",
    "plot_images(images_test_noisy, randomize=False, nrow=1)\n",
    "plot_images(images_recon_ae, randomize=False, nrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark.**\n",
    "To obtain better performance, you may want to train the autoencoder with noisy inputs and clean outputs in the data. This is called *denoising autoencoders* and are very useful to remove noise, provided that you have noisy samples from the same distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to demonstrate the importance of data normalization in PCA using a real-world dataset. \n",
    "\n",
    "This is a dataset from `kaggle` which contains [sales from supermarkets](https://www.kaggle.com/aungpyaeap/supermarket-sales) in three cities in Myanmar. We will import it using the `kaggle` API. Follow the instructions [here](https://github.com/Kaggle/kaggle-api) to set this up. This allows easy downloading of datasets from [https://www.kaggle.com/datasets](https://www.kaggle.com/datasets). Of course, you can also directly download all the data from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "kaggle.api.authenticate()\n",
    "\n",
    "kaggle.api.dataset_download_files(\n",
    "    'aungpyaeap/supermarket-sales',\n",
    "    path='./data',\n",
    "    quiet=False,\n",
    "    unzip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supermarket_data = pd.read_csv('./data/supermarket_sales - Sheet1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supermarket_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x='City', y='Total', data=supermarket_data, hue='Customer type')\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1.1, 0.5), ncol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x='Total', y='Product line', data=supermarket_data, hue='Gender')\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1.1, 0.5), ncol=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us Perform PCA on the supermarket dataset to uncover some hidden relationships. We first remove some attributes for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['Invoice ID', 'Branch', 'Tax 5%', 'Date', 'Time', 'gross margin percentage']:\n",
    "    supermarket_data.pop(c)\n",
    "\n",
    "supermarket_data = pd.get_dummies(supermarket_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supermarket_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we perform PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(supermarket_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(pca.explained_variance_, '-o')\n",
    "plt.axvline(x=14, c='r', ls='--', label=r'$j=14$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalues (explained variance) drops drastically after the 14<sup>th</sup> component. Moreover, the first component *drastically* Dominates the rest. In fact, we can plot a normalized view below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_, '-o')\n",
    "plt.axvline(x=14, c='r', ls='--', label=r'$j=14$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components\n",
    "\n",
    "Let us look at what principal components we obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(15, 12))\n",
    "sns.barplot(x=pca.components_[0], y=supermarket_data.columns, orient='h', ax=ax[0])\n",
    "sns.barplot(x=pca.components_[1], y=supermarket_data.columns, orient='h', ax=ax[1])\n",
    "\n",
    "ax[0].set_title('First Principal Component Axis')\n",
    "ax[1].set_title('Second Principal Component Axis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's wrong? It appears that everything is dominated by either the total price, or the unit price!\n",
    "\n",
    "This make sense, since these numbers are on the order of 10s or 100s, whereas the other features have much smaller values. Then, of course the majority of the variance is going to be dominated by these features. This is undesirable. \n",
    "\n",
    "To resolve this, let us use `sklearn.preprocessing` module to normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "supermarket_data_normalized = (supermarket_data - supermarket_data.min()) / (\n",
    "    supermarket_data.max() - supermarket_data.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we recheck our PCA results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(supermarket_data_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(pca.explained_variance_, '-o')\n",
    "plt.axvline(x=14, c='r', ls='--', label=r'$j=14$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(15, 18))\n",
    "sns.barplot(x=pca.components_[0], y=supermarket_data.columns, orient='h', ax=ax[0])\n",
    "sns.barplot(x=pca.components_[1], y=supermarket_data.columns, orient='h', ax=ax[1])\n",
    "sns.barplot(x=pca.components_[2], y=supermarket_data.columns, orient='h', ax=ax[2])\n",
    "\n",
    "ax[0].set_title('First Principal Component Axis')\n",
    "ax[1].set_title('Second Principal Component Axis')\n",
    "ax[2].set_title('Third Principal Component Axis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA-based Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us demonstrate another usage of PCA. Instead of compression and reconstruction, we are going to use PCA to extract principal features, from which we can build simple recommendation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our dataset, we are going to use the [product SKUs](https://www.kaggle.com/cclark/product-item-data) from an apparel store hosted on kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import kaggle\n",
    "kaggle.api.authenticate()\n",
    "\n",
    "kaggle.api.dataset_download_files(\n",
    "    'cclark/product-item-data',\n",
    "    path='./data',\n",
    "    quiet=False,\n",
    "    unzip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKU_data = pd.read_csv('./data/sample-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKU_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to featurize the description. We will use the handy `TfidVectorizer` from `sklearn.feature_extraction.text`. This implements a bag-of-words embedding, meaning that we count the frequency of English words (and also symbols) at each sentence and have an abstract embedding of the counts. The actual implementation is slightly more complicated. You can read more about that [here](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(SKU_data['description'])\n",
    "X = tfidf_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Sample representation of \\n{SKU_data.iloc[0,1]}')\n",
    "plt.plot(np.asarray(X)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now perform PCA on the extracted text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_)\n",
    "plt.xlabel(r'$j$')\n",
    "plt.ylabel(r'$\\lambda_j$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now transform to latent space. This is a space of important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the distribution of the data in the first three latent dimensions in different combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = pd.DataFrame(\n",
    "    data=latents,\n",
    "    columns=[f'Latent dim {i+1}' for i in range(latents.shape[-1])],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    x='Latent dim 1',\n",
    "    y='Latent dim 2',\n",
    "    data=latents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    x='Latent dim 1',\n",
    "    y='Latent dim 3',\n",
    "    data=latents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    x='Latent dim 2',\n",
    "    y='Latent dim 3',\n",
    "    data=latents,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that we can already see some clustering behaviour, as should be expected since the merchandize descriptions overlap for similar items!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above observations, similar items should have similar principal component scores. So, let us simply take the *distance* in the latent space as a measure of similarity. \n",
    "\n",
    "This allows us to find, given an input sample item description, similar items to it. Consequently, we can make a recommender system from this! \n",
    "\n",
    "For any given input which represent an item a customer purchased, we will recommend some closest neighbour to this in the PCA latent space. Hopefully this gives similar items that the customer might want to look at. Interestingly, we can do this just from the SKU catalogue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(bought_id, num_recommendation=5):\n",
    "    \"\"\"\n",
    "    A simple recommender system using PCA\n",
    "    \"\"\"\n",
    "    print(f'You bought:\\n\\n{SKU_data.description[bought_id][:100]}'+'...')\n",
    "    \n",
    "    # Compute PC scores using only first 10 components\n",
    "    scores = latents.to_numpy()[:, :10]\n",
    "    \n",
    "    # Compute L^2 distances to the input Score\n",
    "    distances = np.sum((scores - scores[bought_id, np.newaxis, :])**2, axis=1)\n",
    "    \n",
    "    # Return recommendations based on closest products in PCA latent space\n",
    "    rec_ids = np.argsort(distances)[1:1+num_recommendation]\n",
    "    rec_desc = SKU_data.description[rec_ids].to_numpy()\n",
    "    print('\\n\\nWe recommend:\\n')\n",
    "    for i, d in zip(rec_ids, rec_desc):\n",
    "        print(f'Id: {i} | Product: {d[:80]+\"...\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend(12)"
   ]
  }
 ],
 "metadata": {
  "author": "",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
