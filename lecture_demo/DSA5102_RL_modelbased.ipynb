{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "\n",
    "In this notebook, we apply model-based algorithms to solve a simple RL problem: FrozenWorld.\n",
    "\n",
    "The main purpose is to introduce the algorithm and also the `OpenAI Gym` environment that allows easy testing of RL algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context('notebook', font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.random.seed(123)  # For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrozenWorld and the OpenAI Gym Environment\n",
    "\n",
    "`OpenAI Gym` is a benchmarking platform for reinforcement learning algorithms. It provides a unified API to specify environments. \n",
    "\n",
    "There are a variety of test problems, including\n",
    "   * Toy problems\n",
    "   * Basic control problems\n",
    "   * Atari games\n",
    "   \n",
    "You can read more about it [here](https://gym.openai.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a specific environment The **Frozen-World**:\n",
    "\n",
    "The environment consists of 4x4=16 tiles of 4 types:\n",
    "   * S = starting point, safe\n",
    "   * F = frozen surface, safe\n",
    "   * H = hole, unsafe\n",
    "   * G = goal\n",
    "   \n",
    "The agent navigates the environment by choosing an action: left, right, up or down.\n",
    "\n",
    "However, the world is *slippery*, so that even if you choose an action, you might not always end up where you are! We will see this later.\n",
    "\n",
    "The goal of the agent is to get to the G (goal) tile, without dropping into the H (hole) tile. When either G or H are reached, the game terminates. The agent receives reward of +1 only when it ends up on a G tile. \n",
    "\n",
    "![Image](https://miro.medium.com/max/842/1*Qp14HWQfOeE2UoSxrxCxAg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create this environment directly using `make`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transition Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the *transition probability*\n",
    "$$\n",
    "    p(s', r| s, a)\n",
    "$$\n",
    "\n",
    "This is stored in `env.P`. Each `env.P[s][a]` is a list, with each element `env.P[s][a][j]` a tuple\n",
    "$$\n",
    "    \\left(\n",
    "        p(s_j,r_j|s,a),\n",
    "        s_j,\n",
    "        r_j,\n",
    "        \\text{Status}\n",
    "    \\right)\n",
    "$$\n",
    "`Status` equals `True` if the episode ends. Otherwise it is `False`.\n",
    "\n",
    "Note that in this game, the actions are organized as \n",
    "```\n",
    "    0: left\n",
    "    1: down\n",
    "    2: right\n",
    "    3: up\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can look at the transition probability at the first state, if we were to go downwards ($a=1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()\n",
    "env.P[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a step, calling the `step` method, which applies the action to the system.\n",
    "\n",
    "The method returns a tuple (next_state, reward, status, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, status, info = env.step(action=1)\n",
    "print(f'State: {state}\\nReward: {reward}\\nStates: {status}\\nInfo: {info}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now go right..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()\n",
    "env.P[state][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, status, info = env.step(action=2)\n",
    "print(f'State: {state}\\nReward: {reward}\\nStates: {status}\\nInfo: {info}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Based Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "Given policy $\\pi$, we evaluate $v_\\pi(s)$ for all $s\\in\\mathcal{S}$.\n",
    "\n",
    "This is done by an iterative solution of the Bellman's equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, gamma, max_iter):\n",
    "    \"\"\"\n",
    "    Evaluate a (deterministic) policy\n",
    "        Here, a policy is an iterable of size nS (number of states)\n",
    "        giving an action to take for each state\n",
    "        \n",
    "        Other stopping criteria are also possible\n",
    "    \"\"\"\n",
    "    value_function = np.zeros(env.nS)\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        tmp = np.zeros_like(value_function)\n",
    "        \n",
    "        # Compute right hand side of Bellman Equation\n",
    "        for state, action in enumerate(policy):\n",
    "            for probablity, next_state, reward, _ in env.P[state][action]:\n",
    "                tmp[state] += probablity * (\n",
    "                    reward + gamma * value_function[next_state])\n",
    "        value_function = tmp\n",
    "\n",
    "    return value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us evaluate the value function of a randomly generated policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.random.choice(range(env.nA), size=(env.nS))\n",
    "value_function = policy_evaluation(env=env, policy=policy, gamma=1.0, max_iter=1000)\n",
    "\n",
    "print(value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "sns.barplot(np.arange(env.nS), value_function, ax=ax[0])\n",
    "sns.heatmap(value_function.reshape(4, 4), ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement\n",
    "\n",
    "Given a policy $\\pi$, we can improve it by computing the *action value function*\n",
    "$$\n",
    "    q_\\pi(s, a) = \\sum_{s',r} p(s',r|s,a)(r + \\gamma v_\\pi(s'))\n",
    "$$\n",
    "\n",
    "Given an action value function, we improve policy by\n",
    "$$\n",
    "    \\pi'(s) = \\mathrm{arg\\,max}_{a} q_\\pi(s, a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_action_value(env, state, value_function, gamma):\n",
    "    \"\"\"\n",
    "    Computes q(state, \\cdot)\n",
    "    \"\"\"\n",
    "    action_values = np.zeros(env.nA)\n",
    "    for action in range(env.nA):\n",
    "        for probablity, next_state, reward, info in env.P[state][action]:\n",
    "            action_values[action] += probablity * (\n",
    "                reward + (gamma * value_function[next_state]))\n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, value_function, gamma):\n",
    "    \"\"\"\n",
    "    Improve a current policy by acting greedily with respect\n",
    "    to the action value function\n",
    "    \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for state in range(env.nS):\n",
    "        action_values = compute_action_value(env, state, value_function, gamma)\n",
    "        policy[state] = np.argmax(action_values)  # pick action to maximize action-value\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the randomized policy as before. To visualize it we define an action map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_map = {\n",
    "    0: '\\u2190',\n",
    "    1: '\\u2193',\n",
    "    2: '\\u2192',\n",
    "    3: '\\u2191',\n",
    "}\n",
    "print(action_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy(policy):\n",
    "    grid = np.array(\n",
    "        [            \n",
    "            ['S', 'F', 'F', 'F'],\n",
    "            ['F', 'H', 'F', 'H'],\n",
    "            ['F', 'F', 'F', 'H'],\n",
    "            ['H', 'F', 'F', 'G'],\n",
    "        ]\n",
    "    )\n",
    "    directions = np.array([action_map[p] for p in policy]).reshape(4, 4)\n",
    "    print(grid)\n",
    "    print(directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_policy = policy_improvement(env=env, value_function=value_function, gamma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_policy(updated_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "Putting things together, we can iterate the steps of policy evaluation and improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, max_iter=100):\n",
    "    \"\"\"\n",
    "    Model-Based Policy Iteration Algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    # intialize a random policy\n",
    "    policy = np.random.choice(range(env.nA), size=(env.nS))\n",
    "    # maintain a copy to check for convergence\n",
    "    policy_prev = np.copy(policy)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        \n",
    "        # evaluate policy\n",
    "        value_function = policy_evaluation(env, policy, 1.0, 100)\n",
    "        \n",
    "        # improve policy\n",
    "        policy = policy_improvement(env, value_function, 1.0)\n",
    "            \n",
    "    return value_function, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_function, policy = policy_iteration(env, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "sns.barplot(np.arange(env.nS), value_function, ax=ax[0])\n",
    "sns.heatmap(value_function.reshape(4, 4), ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_policy(policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
