{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "\n",
    "In this notebook, we investigate some simple examples to illustrate the Probably Approximately Correct (PAC) Framework for studying generalization errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('notebook', font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.random.seed(123)  # For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coin Tossing\n",
    "\n",
    "Let us start with a coin flipping example. Suppose we want to flip a possibly *biased* coin, which comes up heads (1) with probability $p^*$, and tails (0) with probability $1-p^*$.\n",
    "\n",
    "How do we flip a coin on a computer? The easiest way is to generate a random variable\n",
    "$$\n",
    "    x \\sim \\mu \\equiv \\mathcal{U}[0,1]  \n",
    "    \\qquad\n",
    "    \\text{(Uniform Distribution in the unit interval) }\n",
    "$$\n",
    "and return 1 (representing heads) if $x\\leq p^*$, and 0 otherwise.\n",
    "\n",
    "We can write this in terms of an indicator function\n",
    "$$\n",
    "    f^*(x) = \\mathbb{I}_{x\\leq p^*}\n",
    "$$\n",
    "In other words,\n",
    "$$\n",
    "    f^*(x) =\n",
    "    \\begin{cases}\n",
    "        1 & x_i \\leq p^* \\\\\n",
    "        0 & x_i > p^*\n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicator(x, p):\n",
    "    \"\"\"\n",
    "    Returns 1 (Heads) if x < p and 0 (Tails) otherwise    \n",
    "    \"\"\"\n",
    "    return np.where(x < p, np.ones_like(x), np.zeros_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_coin(num_flips, p):\n",
    "    \"\"\"\n",
    "    Flips a coin via generating uniform random variables x\n",
    "    Returns x, y=indicator(x, p)\n",
    "    \"\"\"\n",
    "    x = np.random.uniform(size=(num_flips, ))\n",
    "    return x, indicator(x, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipping Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set $p^*$ to be some fixed value (0.3141593). Let us now flip this \"coin\" by generating a dataset\n",
    "$$\n",
    "    \\mathcal{D} =\n",
    "    \\{\n",
    "        x_i\\sim \\mathcal{U}[0,1],\n",
    "        \\,\n",
    "        y_i = f^*(x_i) = \\mathbb{I}_{x_i \\leq p^*}\n",
    "    \\}_{i=1}^{N}\n",
    "$$\n",
    "\n",
    "Observe that if repeat this for many times, then the average proportion of heads will become close to $p^*$. This is a consequence of the **law of large numbers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_star = 0.3141593\n",
    "f_star = lambda x: indicator(x, p_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_of_heads = []\n",
    "num_flips = [int(10**j) for j in np.linspace(1, 8, 20)]\n",
    "for n in num_flips:\n",
    "    _, flips = flip_coin(num_flips=n, p=p_star)\n",
    "    proportion_of_heads.append(np.mean(flips))\n",
    "coin_data = pd.DataFrame({\n",
    "    'number of flips': num_flips,\n",
    "    'proportion of heads': proportion_of_heads,\n",
    "    r'$p^*$': p_star,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_data.plot(\n",
    "    x='number of flips',\n",
    "    y=['proportion of heads', r'$p^*$'],\n",
    "    logx=True,\n",
    "    style=['-', '--']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm To Return A Consistent $\\hat{f}$\n",
    "\n",
    "Now let us think of an algorithm that guesses $p^*$ from observing the dataset\n",
    "$$\n",
    "    \\mathcal{D} =\n",
    "    \\{\n",
    "        x_i, y_i\n",
    "    \\}_{i=1}^{N}\n",
    "$$\n",
    "\n",
    "There are of course many candidates. For example, you can use the mean $\\hat{p} = \\frac{1}{N}\\sum_{i=1}^{N} y_i$. But this has the disadvantage that the training error is not 0, i.e. it is not *consistent* with the dataset. \n",
    "\n",
    "Instead, we are going to consider another candidate, namely, we pick $\\hat{p}$ to be the largest $x_i$ in our dataset such that $y_i = 1$. In other words\n",
    "\\begin{equation}\\label{eq:p_hat}\n",
    "    \\hat{p} =\n",
    "    \\begin{cases}\n",
    "        \\max \\{ x_i : y_i = 1 \\} & \\text{there exists } y_i = 1 \\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Correspondingly, given the dataset, the above algorithm returns a $\\hat{f}$ given by\n",
    "$$\n",
    "    \\hat{f}(x) = \\mathbb{I}_{x\\leq \\hat{p}}\n",
    "$$\n",
    "with $\\hat{p}$ given by \\eqref{eq:p_hat}.\n",
    "\n",
    "Therefore, in this case our algorithm $\\mathcal{A}$ generates a $\\hat{f}$ from a dataset $\\mathcal{D}$. Symbolically, we can write\n",
    "$$\n",
    "    \\mathcal{A}(\\mathcal{D}) = \\hat{f}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm_p_hat(x, y):\n",
    "    y_heads = y==1.0\n",
    "    if np.all(~y_heads):\n",
    "        return 0.0\n",
    "    else:\n",
    "        return np.max(x[y_heads])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now do some flips and check what $\\hat{p}$ and $\\hat{f}$ looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10  # sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = flip_coin(num_flips=N, p=p_star)\n",
    "p_hat = algorithm_p_hat(x_train, y_train)\n",
    "f_hat = lambda x: indicator(x, p_hat)\n",
    "\n",
    "x_grid = np.linspace(0, 1, 1000)\n",
    "\n",
    "# Training data\n",
    "plt.scatter(x_train[y_train==1], y_train[y_train==1], label=r'$y_i = 1$')\n",
    "plt.scatter(x_train[y_train==0], y_train[y_train==0], label=r'$y_i = 0$')\n",
    "\n",
    "# f*\n",
    "plt.plot(x_grid, f_star(x_grid), '--', label=r'$f^*$', c='k', lw=1)\n",
    "plt.fill_between(x_grid, f_star(x_grid), color='k', alpha=0.1)\n",
    "\n",
    "# f_hat\n",
    "plt.plot(x_grid, f_hat(x_grid), ':', label=r'$\\hat{f}$', c='b', lw=1)\n",
    "plt.fill_between(x_grid, f_hat(x_grid), color='b', alpha=0.1)\n",
    "\n",
    "# Labels\n",
    "plt.text(p_star, 0.25, r'$x=p^*$', color='k')\n",
    "plt.text(p_hat, 0.75, r'$x=\\hat{p}$', color='b')\n",
    "\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Takeaway**\n",
    "\n",
    "Notice the following:\n",
    "1. $\\hat{p}$ and thus $\\hat{f}$ depends on the dataset. If we resample the dataset, they change. In this sense, they are *random variables*\n",
    "2. Despite the randomness, due to our construction, the empirical risk $R_\\mathrm{emp}(\\hat{f})$ is always 0! In other words, our algorithm always returns a *consistent* hypothesis. Moreover, we always have $\\hat{p} \\leq {p}^*$!\n",
    "3. When sample size $N$ increases, $\\hat{p}$ approaches $p^*$. Equivalently, $\\hat{f}$ approaches $f^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Distribution of $R_\\mathrm{pop}(\\hat{f})$\n",
    "\n",
    "Now, let us consider the population risk $R_\\mathrm{pop}(\\hat{f})$. Unlike the empirical risk, this is not zero.\n",
    "\n",
    "Looking at the above figures, it is clear that $f^*$ and $\\hat{f}$ differ only in the range $\\hat{p} < x \\leq p^*$. The population risk is given by\n",
    "\\begin{equation}\n",
    "    R_\\mathrm{pop}(\\hat{f}) \n",
    "    = \\mathbb{E}_{x\\sim\\mu} [\\mathbb{I}_{f^*(x)\\neq \\hat{f}(x)}]\n",
    "    = \\mathbb{P}_{x\\sim\\mu} [\\hat{p} < x \\leq p^*]\n",
    "    = p^* - \\hat{p}.\n",
    "\\end{equation}\n",
    "Again, with each resampling of the training data, $R_\\mathrm{pop}$ is going to be random!\n",
    "\n",
    "Let us now investigate its distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_pop(p_hat, p_star):\n",
    "    \"\"\"\n",
    "    Compute R_pop = p_star - p_hat\n",
    "    \"\"\"\n",
    "    return p_star - p_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_p_hats(num_trials, N):\n",
    "    \"\"\"\n",
    "    Generate p_hats by sampling and training\n",
    "    on datasets of size N\n",
    "    \"\"\"\n",
    "    p_hats = []\n",
    "    for _ in range(num_trials):\n",
    "        x_train, y_train = flip_coin(num_flips=N, p=p_star)\n",
    "        p_hats.append(algorithm_p_hat(x_train, y_train))\n",
    "    return p_hats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for N, a in zip([5, 10, 25], ax):\n",
    "    p_hats = sample_p_hats(num_trials=10000, N=N)\n",
    "    R_pops = [R_pop(p, p_star) for p in p_hats]\n",
    "    \n",
    "    sns.distplot(\n",
    "        R_pops,\n",
    "        kde=False,\n",
    "        norm_hist=True,\n",
    "        bins=50,\n",
    "        ax=a,\n",
    "    )\n",
    "    a.set_title(rf'$N = {N}$ (Worst: {max(R_pops):.2f})')\n",
    "    a.set_xlabel(r'$R_{\\mathrm{pop}}$')\n",
    "    a.set_ylabel('Normalized Frequency')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that even when sample size increases, the worst case $R_\\mathrm{pop}(\\hat{f})$ is bad. It makes better sense to not talk about worse case scenarios. The PAC framework is an example of this: it does not care about worse cases, as long as their probability to occur decays as $N$ increases.\n",
    "\n",
    "More precisely, we want to show that for any error tolerance $\\epsilon$ and worst-case tolerance $\\delta$, we have\n",
    "$$\n",
    "    \\mathbb{P}_{\\mathcal{D}\\sim\\mu}\n",
    "    [\n",
    "        R_\\mathrm{pop}(\\hat{f}) \\leq \\epsilon\n",
    "    ] \\geq 1 - \\delta\n",
    "$$\n",
    "for $N$ *sufficiently* large.\n",
    "\n",
    "Let us solve this problem exactly to get some ideas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Distribution of $\\hat{p}$\n",
    "\n",
    "Let us now compute the distribution of $\\hat{p}$, and thus $R_\\mathrm{pop}(\\hat{f})$.\n",
    "\n",
    "Let us compute the cumulative distribution function\n",
    "\\begin{align*}\n",
    "    &\\mathbb{P}_{\\mathcal{D}\\sim\\mu}[\\hat{p} \\leq z] \\\\\n",
    "    =& \\mathbb{P}_{\\mathcal{D}\\sim\\mu}[x_i \\leq z \\text{ or } x_i > p^* \\text{ for all } i] \\\\\n",
    "    =& (\\mathbb{P}_{x_i\\sim\\mu}[x_i \\leq z \\text{ or } x_i > p^*])^{N}\n",
    "    \\qquad \\text{by i.i.d. assumption} \\\\\n",
    "    =& (1 - p^* + z)^{N}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Our Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_hat_CDF(z, p_star, N):\n",
    "    \"\"\"\n",
    "    Exact CDF P[p_hat <= z]\n",
    "    \"\"\"\n",
    "    return (1 - p_star + z)**N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = np.linspace(0, p_star, 100)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for N, a in zip([5, 15, 30], ax):\n",
    "    p_hats = sample_p_hats(num_trials=10000, N=N)\n",
    "    sns.distplot(p_hats,\n",
    "                 kde=False,\n",
    "                 norm_hist=True,\n",
    "                 bins=50,\n",
    "                 hist_kws=dict(cumulative=True),\n",
    "                 label='Empirical CDF',\n",
    "                 ax=a)\n",
    "    a.plot(x_grid, p_hat_CDF(x_grid, p_star, N=N), label='Exact CDF')\n",
    "    a.set_title(rf'$N = {N}$')\n",
    "    a.set_xlabel(r'$x$')\n",
    "    a.set_ylabel(r'$\\mathbb{P}_{\\mathcal{D}\\sim\\mu}[\\hat{p} \\leq x]$')\n",
    "    #     a.set_ylabel(r'$\\mathbb{P}_{\\mathcal{D}\\sim\\mu}[R_\\mathrm{pop}(\\hat{f}) \\leq x]$')\n",
    "    a.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of $R_\\text{pop}(\\hat{f})$\n",
    "\n",
    "From the distribution of $\\hat{p}$, we can directly get the distribution of $R_{pop} = p^* - \\hat{p}$.\n",
    "\n",
    "We have\n",
    "\\begin{align*}\n",
    "    &\\mathbb{P}_{\\mathcal{D}\\sim\\mu}[R_\\mathrm{pop} \\leq z] \\\\\n",
    "    =&\\mathbb{P}_{\\mathcal{D}\\sim\\mu}[p^* - \\hat{p} \\leq z] \\\\\n",
    "    =&\\mathbb{P}_{\\mathcal{D}\\sim\\mu}[\\hat{p} \\geq p^* - z] \\\\\n",
    "    =& 1 - \\mathbb{P}_{\\mathcal{D}\\sim\\mu}[\\hat{p} < p^* - z] \\\\\n",
    "    =& 1 - \\mathbb{P}_{\\mathcal{D}\\sim\\mu}[\\hat{p} \\leq p^* - z] \n",
    "    + \\mathbb{P}_{\\mathcal{D}\\sim\\mu}[\\hat{p} = p^* - z]\n",
    "\\end{align*}\n",
    "\n",
    "From our previous result, we know that \n",
    "$$\n",
    "    \\mathbb{P}_{\\mathcal{D}\\sim\\mu}[\\hat{p} \\leq p^* - z] = (1-p^*+[p^* - z])^N = (1-z)^N.\n",
    "$$\n",
    "But we have to be a bit careful about $\\mathbb{P}_{\\mathcal{D}\\sim\\mu}[\\hat{p} = p^* - z]$. From the CDF, we know that as long as $z \\neq p^*$, this is going to be zero. If $z = p^*$, then we have $\\mathbb{P}_{\\mathcal{D}\\sim\\mu}[\\hat{p} = 0] = (1-p^*)^N$.\n",
    "\n",
    "Hence, we have\n",
    "\\begin{equation}\n",
    "    \\mathbb{P}_{\\mathcal{D}\\sim\\mu}[R_\\mathrm{pop} \\leq z] =\n",
    "    \\begin{cases}\n",
    "        1 - (1-z)^N & z < p^* \\\\\n",
    "        1 & z = p^*\n",
    "    \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Our Result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_pop_CDF(z, p_star, N):\n",
    "    \"\"\"\n",
    "    Exact CDF P[p_hat <= z]\n",
    "    \"\"\"\n",
    "    return np.where(z==p_star, np.ones_like(z), 1 - (1-z)**N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = np.linspace(0, p_star, 1000)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for N, a in zip([5, 15, 30], ax):\n",
    "    p_hats = sample_p_hats(num_trials=10000, N=N)\n",
    "    R_pops = [p_star-p for p in p_hats]\n",
    "    sns.distplot(R_pops,\n",
    "                 kde=False,\n",
    "                 norm_hist=True,\n",
    "                 bins=50,\n",
    "                 hist_kws=dict(cumulative=True),\n",
    "                 label='Empirical CDF',\n",
    "                 ax=a)\n",
    "    a.plot(x_grid, R_pop_CDF(x_grid, p_star, N=N), label='Exact CDF')\n",
    "    a.set_title(rf'$N = {N}$')\n",
    "    a.set_xlabel(r'$x$')\n",
    "    a.set_ylabel(r'$\\mathbb{P}_{\\mathcal{D}\\sim\\mu}[R_\\mathrm{pop}(\\hat{f}) \\leq x]$')\n",
    "    a.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The PAC Result\n",
    "\n",
    "In this case, we actually have a simple PAC result. Let us assume that $\\epsilon < p^*$ for simplicity, then, we have\n",
    "$$\n",
    "    \\mathbb{P}_{\\mathcal{D}\\sim\\mu}[R_\\mathrm{pop} \\leq \\epsilon] = 1 - (1-\\epsilon)^N\n",
    "$$\n",
    "\n",
    "Thus, we obtain a PAC result if $N$ is large enough so that $(1-\\epsilon)^N \\leq \\delta$, in which case we would have\n",
    "$$\n",
    "    \\mathbb{P}_{\\mathcal{D}\\sim\\mu}[R_\\mathrm{pop} \\leq \\epsilon] \\geq 1 - \\delta\n",
    "$$\n",
    "\n",
    "Solving for $(1-\\epsilon)^N \\leq \\delta $ we get\n",
    "$$\n",
    "    N \\geq \\log \\frac{1}{\\delta} / \\log \\frac{1}{1-\\epsilon}  \\geq \\frac{1}{\\epsilon} \\log \\frac{1}{\\delta}\n",
    "$$\n",
    "In the last line we used the fact that $\\log(z) \\leq 1 + z$ so $-\\log(1/z) \\geq -1 - 1/z$, but this is not terribly important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Test Error Prediction\n",
    "\n",
    "There are other ways to write the previous result. Let us take $N = \\frac{1}{\\epsilon} \\log \\frac{1}{\\delta}$. Then, we have $\\epsilon = \\frac{1}{N} \\log \\frac{1}{\\delta}$.\n",
    "\n",
    "Then, the previous PAC result says that\n",
    "$$\n",
    "    \\mathbb{P}_{\\mathcal{D}\\sim\\mu}\n",
    "    \\left[\n",
    "        R_\\mathrm{pop} \\leq \\frac{1}{N} \\log \\frac{1}{\\delta}\n",
    "    \\right]\n",
    "    \\geq 1 - \\delta\n",
    "$$\n",
    "which says that with high probability, $R_\\mathrm{pop}$ is going to be of order $\\mathcal{O}(1/N)$! \n",
    "\n",
    "In this special case, we can actually compute the expectation of $R_\\mathrm{pop}$ as follows:\n",
    "\\begin{align*}\n",
    "    &\\mathbb{E}_{\\mathcal{D}\\sim\\mu}[R_\\mathrm{pop}(\\hat{f})] \\\\\n",
    "    =& \\int_{0}^{p^*}\n",
    "    \\mathbb{P}_{\\mathcal{D}\\sim\\mu}[R_\\mathrm{pop}(\\hat{f}) > z] dz \\\\\n",
    "    =& \\int_{0}^{p^*} (1 - z)^N dz \\\\\n",
    "    =& \\frac{1 - (1-p^*)^{N+1}}{N+1} \\\\\n",
    "    =& \\mathcal{O}\\left(\\frac{1}{N}\\right)\n",
    "\\end{align*}\n",
    "Therefore, the expected population risk decreases like $1/N$ as the sample size $N$ increases!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Our Result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_repeats = 20\n",
    "\n",
    "test_results = []\n",
    "N_grid = [5, 10, 50, 100, 500, 1000]\n",
    "for N in N_grid:\n",
    "    for _ in range(num_repeats):\n",
    "        x_train, y_train = flip_coin(num_flips=N, p=p_star)\n",
    "        x_test, y_test = flip_coin(num_flips=10000, p=p_star)\n",
    "        p_hat = algorithm_p_hat(x_train, y_train)\n",
    "        f_hat = lambda x: indicator(x, p_hat)\n",
    "        test_results.append([N, np.mean(f_hat(x_test) != y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(test_results, columns=[r'$N$', 'Test Error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(\n",
    "    x=r'$N$',\n",
    "    y='Test Error',\n",
    "    data=test_results,\n",
    "    label=r'Test Error vs $N$'\n",
    ")\n",
    "ax.set(xscale='log', yscale='log')\n",
    "ax.loglog(N_grid, [1/N for N in N_grid], '--', label=r'$1/N$')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Generalization Bound for Finite Hypothesis Space\n",
    "\n",
    "Now let us look at why we should expect some universal generalization bounds for finite hypothesis space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to define a class of arbitrary binary classifiers based on a Gaussian RBF function. The class below implements this.\n",
    "\n",
    "Out of this finite set of RBF classifiers, we are going to pick one to be our oracle $f^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRBFHypothesisSpace(object):\n",
    "    def __init__(self, num_h):\n",
    "        self.num_h = num_h\n",
    "        self.mean = np.random.uniform(size=(num_h, 2))\n",
    "        self.cov = np.random.uniform(size=(num_h, 2, 2))\n",
    "        self.cov_inv = np.asarray([\n",
    "            np.linalg.inv(c.T @ c + 0.1 * np.identity(c.shape[0]))\n",
    "            for c in self.cov\n",
    "        ])\n",
    "        self.j_star = np.random.choice(range(num_h))\n",
    "\n",
    "    def hxj(self, x, j):\n",
    "        norm = np.sum(\n",
    "            (x - self.mean[j]) @ self.cov_inv[j] * (x - self.mean[j]), axis=1)\n",
    "        pdf = np.exp(-norm)\n",
    "        return (np.sign(pdf - 0.5) + 1) / 2\n",
    "\n",
    "    def f_star(self, x):\n",
    "        return self.hxj(x, self.j_star)\n",
    "\n",
    "    def h(self, j):\n",
    "        return lambda x: self.hxj(x, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heat_map(h, ax):\n",
    "    x1_mesh, x2_mesh = np.mgrid[0:1:100j, 0:1:100j]\n",
    "    positions = np.vstack([x1_mesh.ravel(), x2_mesh.ravel()]).T\n",
    "    values = h(positions)\n",
    "    ax.contourf(np.linspace(0, 1, 100), np.linspace(0, 1, 100), values.reshape(100, 100), cmap='GnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = RandomRBFHypothesisSpace(num_h=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the optimal $f^* \\in \\mathcal{H}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heat_map(inds.f_star, plt.axes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us plot the other $h \\in \\mathcal{H}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "num_plots = 15\n",
    "num_plots = min(num_plots, inds.num_h)\n",
    "\n",
    "num_cols = 5\n",
    "num_rows = ceil(num_plots / num_cols)\n",
    "\n",
    "fig, ax = plt.subplots(num_rows, num_cols, figsize=(5*num_cols, 4*num_rows))\n",
    "\n",
    "for a in ax.ravel():\n",
    "    j = np.random.choice(range(inds.num_h))\n",
    "    plot_heat_map(inds.h(j), ax=a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency vs Increasing Data Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us sample $N$ data points from $f^*$ according to the uniform distribution in the unit square. However, note that this would work with *any* distribution (Try it yourself as an exercise!)\n",
    "\n",
    "Suppose our algorithm returns a consistent hypothesis, meaning that it must have zero training error. What happens when you increase the number of data points?\n",
    "\n",
    "Let us investigate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly draw $N$ samples, uniformly in the unit square. This builds the dataset\n",
    "$$\n",
    "    \\mathcal{D} = \\{ x_i \\sim \\mu\\equiv\\mathcal{U}[0,1]^2 , y_i = f^*(x_i) \\}_{i=1}^{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "\n",
    "x_train = np.random.uniform(size=(N, 2))\n",
    "y_train = inds.f_star(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "plot_heat_map(inds.f_star, ax=ax)\n",
    "positives = y_train == 1.0\n",
    "ax.scatter(x_train[:, 1][positives], x_train[:, 0][positives], c='lightblue', label='1')\n",
    "ax.scatter(x_train[:, 1][~positives], x_train[:, 0][~positives], c='green', label='0')\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(bbox_to_anchor=(1.1, 1.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us plot all the $h\\in\\mathcal{H}$ which are consistent with the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consistency_check(h, x, y):\n",
    "    y_pred = h(x)\n",
    "    return np.all(y_pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots = 15\n",
    "\n",
    "check_results = [consistency_check(inds.h(j), x_train, y_train) for j in range(inds.num_h)]\n",
    "consistent_subset = np.arange(inds.num_h)[check_results]\n",
    "print(f'Number of consistent hypotheses: {len(consistent_subset)}')\n",
    "\n",
    "num_plots = min(num_plots, len(consistent_subset))\n",
    "\n",
    "num_cols = 5\n",
    "num_rows = ceil(num_plots / num_cols)\n",
    "num_cols = min(num_cols, len(consistent_subset))\n",
    "\n",
    "fig, ax = plt.subplots(num_rows, num_cols, figsize=(5*num_cols, 4*num_rows), squeeze=False)\n",
    "\n",
    "for a in ax.ravel():\n",
    "    j = np.random.choice(consistent_subset)\n",
    "    plot_heat_map(inds.h(j), ax=a)\n",
    "    a.scatter(x_train[:, 1][positives], x_train[:, 0][positives], c='lightblue', label='1')\n",
    "    a.scatter(x_train[:, 1][~positives], x_train[:, 0][~positives], c='green', label='0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happpens when we increase $N$?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
