{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "\n",
    "In this notebook, we implement a simple Q-learning algorithm to solve the (in)famous handphone game [*Flappy Bird*](https://en.wikipedia.org/wiki/Flappy_Bird)\n",
    "\n",
    "The main takeaways are\n",
    "  * Implementation of Q-learning\n",
    "  * Using the Python Learning Environment (PLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # Nice progress bar, optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to first install the PyGame Learning Environment. Installation instructions can be found at the [Github Repository](https://github.com/ntasfi/PyGame-Learning-Environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE\n",
    "np.random.seed(123)  # For Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we make the game environment and the PLE environment that will be driving the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_env = FlappyBird()\n",
    "ple_env = PLE(game_env, fps=30, display_screen=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore the game interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ple_env.init()  # initalizes the game state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `getGameState()` method obtains a dictionary describing the state of the game. This forms the basis of the state description $s$ that will be used in our reinforcement learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_env.getGameState()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current score can be obtained by the `score()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ple_env.score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The available action set is obtained by the `getActionSet`\n",
    "\n",
    "In the case of Flappy Bird, the actions are\n",
    "* 119: press up (jump)\n",
    "* None: do nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ple_env.getActionSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can play the game by supplying an input action taken from the action set, by calling the `act()` method. The method returns the current score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ple_env.act(action=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The status of the game can be queried by the game_over() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ple_env.game_over()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment/Reward Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply reinforcement learning on this problem. \n",
    "\n",
    "The first step will be deciding a suitable state representation.\n",
    "\n",
    "The raw state is returned by `getGameState()`, which has a lot of information:\n",
    "\n",
    "```python\n",
    "print(game_env.getGameState())\n",
    "\n",
    "{'player_y': 256,\n",
    " 'player_vel': 0,\n",
    " 'next_pipe_dist_to_player': 309.0,\n",
    " 'next_pipe_top_y': 53,\n",
    " 'next_pipe_bottom_y': 153,\n",
    " 'next_next_pipe_dist_to_player': 453.0,\n",
    " 'next_next_pipe_top_y': 153,\n",
    " 'next_next_pipe_bottom_y': 253}\n",
    "```\n",
    "\n",
    "We will take 3 relevant features from these descriptions\n",
    "1. The distance to next pipe\n",
    "2. The height difference between the top pipe and bird\n",
    "3. The height difference between the bird and bottom pipe\n",
    "\n",
    "For each of these features, we will bin and summarize the state as one of the possible bins.\n",
    "For example, the heigh difference between the top pipe and bird can be any value between -512, and 512. Hence, we will bin these values into bins of width `bin_size`, which results in a total of `1024//bin_size` possible states for this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = 10\n",
    "n_bins_dist = np.ceil(350 / bin_size).astype('int')\n",
    "n_bins_top = np.ceil(1024 / bin_size).astype('int')\n",
    "n_bins_bot = np.ceil(1024 / bin_size).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write the follow function that converts a raw game state into one with the above three features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_state(game_state):\n",
    "    state = np.zeros((3, ), dtype=int)\n",
    "    state[0] = game_state['next_pipe_dist_to_player'] // bin_size\n",
    "    state[1] = (game_state['next_pipe_top_y'] - game_state['player_y'] +\n",
    "                512) // bin_size\n",
    "    state[2] = (game_state['next_pipe_bottom_y'] - game_state['player_y'] +\n",
    "                512) // bin_size\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_state = game_env.getGameState()\n",
    "game_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_state(game_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need some parser for the reward, since the score need not be the actual reward/return we feed into our RL algorithm.\n",
    "\n",
    "For example, in this case we saw that the `score()` method returns -5 if the game is lost, +1 for every pipe passed and 0 otherwise.\n",
    "\n",
    "We will modify this so that we return +1 for each time we do not hit a pipe and  -100 if we hit the pipe or floor.\n",
    "\n",
    "**Remark.** Do experiment with different reward definitions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_reward(game_reward):\n",
    "    return 1.0 if game_reward>= 0 else -100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement Q-Learning.\n",
    "\n",
    "The $Q$ function (action-value function) is now regarded as a tensor with dimensions\n",
    "\n",
    "    [n_bins_dist, n_bins_top, n_bins_bot, 2]\n",
    "\n",
    "where the first 3 dimensions describes the state and the last dimension describes the action. The first action is \n",
    "\n",
    "We first implement a action choice function which picks an action deterministically based on the Bellman's optimality principle:\n",
    "$$\n",
    "    \\pi(s) = \\mathrm{arg\\,max}_a Q(s, a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, Q):\n",
    "    q_up, q_nothing = Q[tuple(state)]\n",
    "    return 0 if q_up > q_nothing else 1\n",
    "\n",
    "action_dict = {0: 119, 1: None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we write a function to update the Q matrix, according to the usual Q learning formula\n",
    "\n",
    "$$\n",
    "    Q(s,a)\n",
    "    \\leftarrow\n",
    "    (1-\\alpha) \\underbrace{Q(s,a)}_{Q_{\\text{old}}}\n",
    "    +\n",
    "    \\alpha\n",
    "    [\n",
    "        \\underbrace{r + \\gamma \\max_{a'} Q(s', a')}_{Q_{\\text{new}}}\n",
    "    ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(Q, state, next_state, action, reward, alpha, gamma):\n",
    "    Q_new = reward + gamma * np.max(Q[tuple(next_state)])\n",
    "    Q_old = Q[tuple(state)][action]\n",
    "    Q[tuple(state)][action] = (1 - alpha) * Q_old + alpha * Q_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize Q as all zeros and train using Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros((n_bins_dist, n_bins_top, n_bins_bot, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "returns = []\n",
    "alpha = 0.1\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "\n",
    "for _ in tqdm(range(n_epochs)):\n",
    "    ple_env.init()\n",
    "    state = parse_state(game_env.getGameState())\n",
    "    cumulative_reward = 0.0\n",
    "    \n",
    "    while not ple_env.game_over():\n",
    "        # Sample action\n",
    "        action = choose_action(state, Q)\n",
    "        \n",
    "        # Evolve state and parse reward\n",
    "        game_reward = ple_env.act(action_dict[action])\n",
    "        reward = parse_reward(game_reward)\n",
    "        next_state = parse_state(game_env.getGameState())\n",
    "        \n",
    "        # Update Q function\n",
    "        update_Q(Q, state, next_state, action, reward, alpha, gamma)\n",
    "        cumulative_reward += reward\n",
    "        state = next_state\n",
    "        time.sleep(0.01)  # Comment out if no need to visualize\n",
    "    \n",
    "    scores.append(ple_env.score())\n",
    "    returns.append(cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax0.plot(scores, '-')\n",
    "ax1.plot(returns, ':')\n",
    "\n",
    "ax0.set_title('Scores')\n",
    "ax1.set_title('Returns')\n",
    "\n",
    "for ax in (ax0, ax1):\n",
    "    ax.set_xlabel('Episode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Try to improve the performance by, e.g.\n",
    "1. Changing the reward function\n",
    "2. Changing the initialization\n",
    "3. Changing how the state is parsed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
